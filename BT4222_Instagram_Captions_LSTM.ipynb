{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546f1deb",
   "metadata": {},
   "source": [
    "# ================ LSTM Model Train & Test Notebook =============== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install demoji\n",
    "# !pip install spacy\n",
    "# !pip install clean-text\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import spacy\n",
    "import demoji\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, TextVectorization, InputLayer, Bidirectional\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66891a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db2c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('captions_df.csv')\n",
    "df = df.iloc[:, 1:4]\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e86995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counts of number of post and number of captions (Post can have no captions)\n",
    "\n",
    "df['num_of_post'] = [0] * len(df['captions'])\n",
    "df['num_of_captions'] = [0] * len(df['captions'])\n",
    "\n",
    "y = df['captions'].apply(lambda x: ast.literal_eval(x))\n",
    "y = y.apply(lambda cap_list: [cap for cap in cap_list if cap is not None])\n",
    "df['captions'] = y\n",
    "for i in range(len(df['captions'])):\n",
    "    x = df['captions'][i]\n",
    "    df.loc[i, 'num_of_post'] = len(x)\n",
    "    df.loc[i, 'num_of_captions'] = sum(cap != None for cap in x)\n",
    "\n",
    "dfe = df.explode('captions').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d7059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(dfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CountVectorizer()\n",
    "tokenizer.fit_transform(dfe['captions'].dropna())\n",
    "word_index = tokenizer.vocabulary_\n",
    "print(f\"No. of unique words: {len(word_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299054a4",
   "metadata": {},
   "source": [
    "### Clean Up Captions (E.g remove emoji, new lines, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe['captions'] = dfe['captions'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_em(text):\n",
    "    dem = demoji.findall(text)\n",
    "    for item in dem.keys():\n",
    "        text = text.replace(item, '')\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    if pd.notnull(text):\n",
    "        text = re.sub(r\"(\\n)+\", \" \", text)\n",
    "        text = remove_em(text)\n",
    "        text = text.lower() # lowercase text\n",
    "        text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "        text = ' '.join([ps.stem(w) for w in text.split()])\n",
    "        text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "        text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "        text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    else:\n",
    "        text = ''\n",
    "    \n",
    "    return text\n",
    "\n",
    "lstm_df = dfe.copy()\n",
    "lstm_df['captions'] = lstm_df['captions'].apply(clean_text)\n",
    "lstm_df['captions_length'] = [len(cap.split()) for cap in lstm_df['captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf8a43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"EXAMPLE OF AN EMPTY CAPTION POST BY A BOT: \")\n",
    "display(lstm_df[lstm_df['username'] == 'breely_wilkey'])\n",
    "\n",
    "print(\"EXAMPLES OF CAPTION POST BY A BOT: \")\n",
    "display(lstm_df[lstm_df['username'] == 'copy ai'])\n",
    "\n",
    "print(\"EXAMPLES OF CAPTION POST BY A REAL PERSON: \")\n",
    "display(lstm_df[lstm_df['username'] == 'ryanxgo'])\n",
    "\n",
    "print('Shape of lstm_df:', lstm_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(lstm_df['captions_length'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05b7be",
   "metadata": {},
   "source": [
    "## LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c69b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS_NUM = 5000  # Maximum vocab size.\n",
    "MAX_SEQUENCE_LEN = 250  # Sequence length to pad the outputs to.\n",
    "EMBEDDING_DIMS = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_TOKENS_NUM, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(lstm_df['captions'])\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"No. of unique words: {len(word_index)}\")\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(lstm_df['captions'])\n",
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(train_padded, lstm_df['is_fake'], test_size = 0.20, stratify = lstm_df['is_fake'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size = 0.20, stratify = y_train_val)\n",
    "\n",
    "# Oversampling\n",
    "fake_captions_train = X_train[y_train == 1]\n",
    "real_captions_train = X_train[y_train == 0]\n",
    "fake_label_train = y_train[y_train == 1].reset_index(drop = True)\n",
    "real_label_train = y_train[y_train == 0].reset_index(drop = True)\n",
    "\n",
    "ids = np.arange(len(fake_captions_train))\n",
    "choices = np.random.choice(ids, len(real_captions_train))\n",
    "\n",
    "res_pos_features = fake_captions_train[choices]\n",
    "res_pos_labels = fake_label_train[choices]\n",
    "\n",
    "resampled_features = np.concatenate([res_pos_features, real_captions_train], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, real_label_train], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "X_train = resampled_features\n",
    "y_train = pd.get_dummies(resampled_labels).values\n",
    "y_valid = pd.get_dummies(y_valid).values\n",
    "y_test = pd.get_dummies(y_test).values\n",
    "\n",
    "print(\"==========\")\n",
    "print(f\"Length of train set is {len(y_train)}\")\n",
    "print(f\"Length of validation set is {len(y_valid)}\")\n",
    "print(f\"Length of test set is {len(y_test)}\")\n",
    "print(\"==========\")\n",
    "\n",
    "model1 = Sequential([\n",
    "    Embedding(5000, EMBEDDING_DIMS),\n",
    "    LSTM(EMBEDDING_DIMS),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.TrueNegatives()])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history1 = model1.fit(X_train, y_train, epochs=epochs, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "model1.evaluate(X_test, y_test, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d164230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "model_num = 1\n",
    "\n",
    "ax2.plot(history1.history['loss'])\n",
    "ax2.plot(history1.history['val_loss'])\n",
    "ax2.set_title(f\"Model {model_num} Loss\")\n",
    "ax2.set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "ax2.legend(['loss', 'val_loss'])\n",
    "\n",
    "ax1.plot(history1.history['recall'])\n",
    "ax1.plot(history1.history['val_recall'])\n",
    "ax1.set_title(f\"Model {model_num} Recall\")\n",
    "ax1.set(xlabel=\"Epochs\", ylabel=\"Recall\")\n",
    "ax1.legend(['recall', 'val_recall'])\n",
    "\n",
    "total_train_fake = [x + y for x, y in zip(history1.history['false_positives'], history1.history['true_negatives'])]\n",
    "total_val_fake = [x + y for x, y in zip(history1.history['val_false_positives'], history1.history['val_true_negatives'])]\n",
    "ax3.plot([i/j for i, j in zip(history1.history['false_positives'],total_train_fake)])\n",
    "ax3.plot([i/j for i, j in zip(history1.history['val_false_positives'],total_val_fake)])\n",
    "ax3.set_title(f\"Model {model_num} FPR\")\n",
    "ax3.set(xlabel=\"Epochs\", ylabel=\"FPR\")\n",
    "ax3.legend(['FPR', 'val_fpr'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58c2f0",
   "metadata": {},
   "source": [
    "## LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_NUM = 5000  # Maximum vocab size.\n",
    "MAX_SEQUENCE_LEN = 250  # Sequence length to pad the outputs to.\n",
    "EMBEDDING_DIMS = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_TOKENS_NUM, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(lstm_df['captions'])\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"No. of unique words: {len(word_index)}\")\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(lstm_df['captions'])\n",
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(train_padded, lstm_df['is_fake'], test_size = 0.20, stratify = lstm_df['is_fake'], random_state = 42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size = 0.20, stratify = y_train_val, random_state = 42)\n",
    "\n",
    "# Oversampling\n",
    "fake_captions_train = X_train[y_train == 1]\n",
    "real_captions_train = X_train[y_train == 0]\n",
    "fake_label_train = y_train[y_train == 1].reset_index(drop = True)\n",
    "real_label_train = y_train[y_train == 0].reset_index(drop = True)\n",
    "\n",
    "ids = np.arange(len(fake_captions_train))\n",
    "choices = np.random.choice(ids, len(real_captions_train))\n",
    "\n",
    "res_pos_features = fake_captions_train[choices]\n",
    "res_pos_labels = fake_label_train[choices]\n",
    "\n",
    "resampled_features = np.concatenate([res_pos_features, real_captions_train], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, real_label_train], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "X_train = resampled_features\n",
    "y_train = pd.get_dummies(resampled_labels).values\n",
    "y_valid = pd.get_dummies(y_valid).values\n",
    "y_test = pd.get_dummies(y_test).values\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(MAX_TOKENS_NUM, EMBEDDING_DIMS),\n",
    "    Bidirectional(LSTM(EMBEDDING_DIMS)),\n",
    "    Dense(EMBEDDING_DIMS, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.TrueNegatives()])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "model.evaluate(X_test, y_test, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39460392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "model_num = 2\n",
    "\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.set_title(f\"Model {model_num} Loss\")\n",
    "ax2.set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "ax2.legend(['loss', 'val_loss'])\n",
    "\n",
    "ax1.plot(history.history['recall_1'])\n",
    "ax1.plot(history.history['val_recall_1'])\n",
    "ax1.set_title(f\"Model {model_num} Recall\")\n",
    "ax1.set(xlabel=\"Epochs\", ylabel=\"Recall\")\n",
    "ax1.legend(['recall', 'val_recall'])\n",
    "\n",
    "total_train_fake = [x + y for x, y in zip(history.history['false_positives_1'], history.history['true_negatives_1'])]\n",
    "total_val_fake = [x + y for x, y in zip(history.history['val_false_positives_1'], history.history['val_true_negatives_1'])]\n",
    "ax3.plot([i/j for i, j in zip(history.history['false_positives_1'],total_train_fake)])\n",
    "ax3.plot([i/j for i, j in zip(history.history['val_false_positives_1'],total_val_fake)])\n",
    "ax3.set_title(f\"Model {model_num} FPR\")\n",
    "ax3.set(xlabel=\"Epochs\", ylabel=\"FPR\")\n",
    "ax3.legend(['FPR', 'val_fpr'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665151cb",
   "metadata": {},
   "source": [
    "## LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f98419",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_NUM = 5000  # Maximum vocab size.\n",
    "MAX_SEQUENCE_LEN = 250  # Sequence length to pad the outputs to.\n",
    "EMBEDDING_DIMS = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_TOKENS_NUM, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(lstm_df['captions'])\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"No. of unique words: {len(word_index)}\")\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(lstm_df['captions'])\n",
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(train_padded, lstm_df['is_fake'], test_size = 0.20, stratify = lstm_df['is_fake'], random_state = 42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size = 0.20, stratify = y_train_val, random_state = 42)\n",
    "\n",
    "# Oversampling\n",
    "fake_captions_train = X_train[y_train == 1]\n",
    "real_captions_train = X_train[y_train == 0]\n",
    "fake_label_train = y_train[y_train == 1].reset_index(drop = True)\n",
    "real_label_train = y_train[y_train == 0].reset_index(drop = True)\n",
    "\n",
    "ids = np.arange(len(fake_captions_train))\n",
    "choices = np.random.choice(ids, len(real_captions_train))\n",
    "\n",
    "res_pos_features = fake_captions_train[choices]\n",
    "res_pos_labels = fake_label_train[choices]\n",
    "\n",
    "resampled_features = np.concatenate([res_pos_features, real_captions_train], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, real_label_train], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "X_train = resampled_features\n",
    "y_train = pd.get_dummies(resampled_labels).values\n",
    "y_valid = pd.get_dummies(y_valid).values\n",
    "y_test = pd.get_dummies(y_test).values\n",
    "\n",
    "model3 = Sequential([\n",
    "    Embedding(MAX_TOKENS_NUM, EMBEDDING_DIMS),\n",
    "    Bidirectional(LSTM(EMBEDDING_DIMS)),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.TrueNegatives()])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, epochs=epochs, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76764b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c83618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test set\n",
    "model3.evaluate(X_test, y_test, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "model_num = 3\n",
    "\n",
    "ax2.plot(history3.history['loss'])\n",
    "ax2.plot(history3.history['val_loss'])\n",
    "ax2.set_title(f\"Model {model_num} Loss\")\n",
    "ax2.set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "ax2.legend(['loss', 'val_loss'])\n",
    "\n",
    "ax1.plot(history3.history['recall_2'])\n",
    "ax1.plot(history3.history['val_recall_2'])\n",
    "ax1.set_title(f\"Model {model_num} Recall\")\n",
    "ax1.set(xlabel=\"Epochs\", ylabel=\"Recall\")\n",
    "ax1.legend(['recall', 'val_recall'])\n",
    "\n",
    "total_train_fake = [x + y for x, y in zip(history3.history['false_positives_2'], history3.history['true_negatives_2'])]\n",
    "total_val_fake = [x + y for x, y in zip(history3.history['val_false_positives_2'], history3.history['val_true_negatives_2'])]\n",
    "ax3.plot([i/j for i, j in zip(history3.history['false_positives_2'],total_train_fake)])\n",
    "ax3.plot([i/j for i, j in zip(history3.history['val_false_positives_2'],total_val_fake)])\n",
    "ax3.set_title(f\"Model {model_num} FPR\")\n",
    "ax3.set(xlabel=\"Epochs\", ylabel=\"FPR\")\n",
    "ax3.legend(['FPR', 'val_fpr'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664aaa07",
   "metadata": {},
   "source": [
    "# Extra Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = ['NOT FAKE', 'FAKE']\n",
    "\n",
    "cleaned_text = clean_text(\"Happy to meet Governor of South Australia Frances Adamson this afternoon. We reaffirmed the excellent relations between Singapore and Australia, as well as close economic and cultural ties between Singapore and South Australia. South Australia and its capital city of Adelaide have been the torchbearer for innovation and new ideas in areas such as renewable energy, food, education as well as arts and culture. I look forward to further cooperation with South Australia in these areas.\")\n",
    "example = tokenizer.texts_to_sequences([cleaned_text])\n",
    "example_pad = pad_sequences(example, maxlen=30, padding='post', truncating='post')\n",
    "pred = np.argmax(model.predict(example_pad), axis = -1)\n",
    "\n",
    "for p in pred:\n",
    "    print(lab[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d580b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(train_padded[10]))\n",
    "print('---')\n",
    "print(lstm_df['captions'][10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
